{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "from attnset import ProbabilityAttention, FullAttention\n",
    "from pretreatment import PositionalEmbedding, ExpandEmbedding\n",
    "from module import Distilling, CrossLayer\n",
    "from moduledata import AceUnitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAceData:\n",
    "    def __init__(self, source_data: pd.DataFrame, verify_size: int, test_size: int, unit_size: int, predict_size: int, emd_col: list, result_col: list, back_num: int):\n",
    "        super(AllAceData, self).__init__()\n",
    "        dataframe = source_data\n",
    "        self.true_train_set,self.true_verify_set,self.true_test_set = [], [], []\n",
    "        self.former_train_set,self.former_verify_set,self.former_test_set = [], [], []\n",
    "\n",
    "        split_index = - back_num * test_size - verify_size - 1\n",
    "        train_tmp = dataframe.iloc[:split_index].reset_index(drop=True)\n",
    "        self.true_train_set.append(train_tmp)\n",
    "        self.former_train_set.append(AceUnitData(train_tmp, unit_size, predict_size, emd_col=emd_col, result_col=result_col))\n",
    "\n",
    "        for _ in range(back_num):\n",
    "            # train data\n",
    "            train_tmp = dataframe.iloc[split_index-unit_size+predict_size: split_index+test_size if split_index<-test_size else -1].reset_index(drop=True)\n",
    "            self.true_train_set.append(train_tmp)\n",
    "            self.former_train_set.append(AceUnitData(train_tmp, unit_size, predict_size, emd_col=emd_col, result_col=result_col))\n",
    "\n",
    "            # verify data\n",
    "            split_index += verify_size\n",
    "            verify_tmp = dataframe.iloc[split_index-verify_size-unit_size+predict_size: split_index].reset_index(drop=True)\n",
    "            self.true_verify_set.append(verify_tmp)\n",
    "            self.former_verify_set.append(AceUnitData(verify_tmp, unit_size, predict_size, emd_col=emd_col, result_col=result_col))\n",
    "\n",
    "            # test data\n",
    "            split_index += test_size\n",
    "            test_tmp = dataframe.iloc[split_index-test_size-unit_size+predict_size: split_index].reset_index(drop=True)\n",
    "            self.true_test_set.append(test_tmp)\n",
    "            self.former_test_set.append(AceUnitData(test_tmp, unit_size, predict_size, emd_col=emd_col, result_col=result_col))\n",
    "            \n",
    "            # index\n",
    "            split_index -= verify_size\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.former_train_set, self.former_verify_set, self.former_test_set\n",
    "\n",
    "    def get_not_normaliza_data(self):\n",
    "        return self.true_train_set, self.true_verify_set, self.true_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACEFormer(nn.Module):\n",
    "    def __init__(self, data_dim: int, embed_dim: int, forward_dim: int, unit_size: int, dis_layer: int = 3, attn_layer: int = 2, factor: int = 5, dropout: float = 0.1, activation: str = \"relu\"):\n",
    "        super(ACEFormer, self).__init__()\n",
    "        self.dis_layer = dis_layer\n",
    "        self.attn_layer = attn_layer\n",
    "\n",
    "        # pretreatment module\n",
    "        ## data embedding\n",
    "        self.ExpandConv = ExpandEmbedding(data_dim, embed_dim)\n",
    "        ## local position\n",
    "        self.position_emb = PositionalEmbedding(embed_dim)\n",
    "        ## dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # distillation module\n",
    "        ## temporal perception mechanism\n",
    "        self.temporal = nn.ParameterList()\n",
    "        ## distillation mechanism\n",
    "        self.dis_attn = nn.ModuleList()\n",
    "        self.distill = nn.ModuleList()\n",
    "        ## create distillation module\n",
    "        for num in range(dis_layer):\n",
    "            embed_tmp = embed_dim // pow(2, num)\n",
    "            self.dis_attn.append(\n",
    "                CrossLayer(\n",
    "                    ProbabilityAttention(embed_tmp, n_heads=8, factor=factor),\n",
    "                    embed_dim=embed_tmp, forward_dim=forward_dim, \n",
    "                    dropout=dropout, activation=activation\n",
    "                )\n",
    "            )\n",
    "            self.distill.append(Distilling(embed_tmp))\n",
    "            self.temporal.append(nn.Parameter(torch.rand(unit_size, embed_tmp // 2)))\n",
    "\n",
    "        # attention module\n",
    "        self.attn = nn.ModuleList(\n",
    "            CrossLayer(\n",
    "                FullAttention(embed_tmp // 2, n_heads=8, factor=factor),\n",
    "                embed_dim=embed_tmp // 2, forward_dim=embed_tmp * 2,\n",
    "                dropout=dropout, activation=activation\n",
    "            ) for _ in range(attn_layer)\n",
    "        )\n",
    "\n",
    "        # projection\n",
    "        self.full_connect = nn.Linear(embed_tmp // 2, 1, bias=True)\n",
    "\n",
    "    def forward(self, data: torch.tensor):\n",
    "        # data embedding\n",
    "        data_emb = self.ExpandConv(data)\n",
    "        local_position = self.position_emb(data)\n",
    "        dis_input = data_emb + local_position\n",
    "        dis_output = self.dropout(dis_input)\n",
    "\n",
    "        # distilling module\n",
    "        for i in range(self.dis_layer):\n",
    "            attn_res = self.dis_attn[i](dis_output, dis_output)\n",
    "            dis_res = self.distill[i](attn_res)\n",
    "            dis_output = dis_res + self.temporal[i]\n",
    "        \n",
    "        # attention dealt\n",
    "        attn_output = dis_output\n",
    "        for layer in self.attn:\n",
    "            attn_output = layer(attn_output, dis_output)\n",
    "\n",
    "        # projection\n",
    "        output = self.full_connect(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data: Data.Dataset, batch_size: int, device: str = \"cpu\", iteration: int = 2000):\n",
    "    # data to DataLoader\n",
    "    train_loader = Data.DataLoader(train_data, batch_size)\n",
    "    # loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = opt.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # trainning model\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    for epoch in range(iteration):\n",
    "        ## calculate the loss\n",
    "        batch_count, loss_count = 0, 0.0\n",
    "\n",
    "        for data, _, true_data in train_loader:\n",
    "            data = data.float().to(device)\n",
    "            true_data = true_data.float().to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, true_data)\n",
    "            loss_count += loss.cpu().data\n",
    "            batch_count += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            end = time.time()\n",
    "            print(\"epoch=\" + str(epoch) + \", loss=\" + str(loss_count/batch_count) + \", use time=\" + str(int(end - start)) + \"s, in \" + time.strftime(\"%m-%d %H:%M:%S\", time.localtime()) + \", predict next epoch in \" + time.strftime(\"%m-%d %H:%M:%S\", time.localtime(time.time() + end - start)))\n",
    "            start = time.time()\n",
    "\n",
    "    return model\n",
    "\n",
    "def test(model, test_data, predict_size: int, device: str):\n",
    "    model.eval()\n",
    "    true, predict = [], []\n",
    "    with torch.no_grad():\n",
    "        for (data, stamp, true_data) in test_data:\n",
    "            data = torch.tensor(data).unsqueeze(0).float().to(device)\n",
    "            stamp = torch.tensor(stamp).unsqueeze(0).int().to(device)\n",
    "\n",
    "            true.append(true_data[-predict_size])\n",
    "            outputs = model(data)\n",
    "            predict.append(outputs.reshape(-1)[-predict_size:].tolist())\n",
    "\n",
    "    true, predict = test_data.anti_normalize_data(np.array(true), np.array(predict))\n",
    "    return true, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process and GPU use\n",
    "device = \"cpu\"\n",
    "# experiment times for each data\n",
    "model_time = 1\n",
    "# data file\n",
    "data_path = \"stockdata/NDX100.csv\"\n",
    "# the number of the dataset splits\n",
    "backtest_num = 1\n",
    "# iteration number\n",
    "iteration = 1\n",
    "# data\n",
    "source_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "batch_size = 64\n",
    "emd_col = ['close', 'close_x', 'close_y', 'vol', 'vol_x', 'vol_y']\n",
    "result_col = ['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create dataset\n",
      "create model\n"
     ]
    }
   ],
   "source": [
    "# product dataset for model\n",
    "print(\"create dataset\")\n",
    "data_set = AllAceData(source_data=source_data, verify_size=50, test_size=100, unit_size=30, predict_size=5, emd_col=emd_col, result_col=result_col, back_num=backtest_num)\n",
    "former_train_set, former_verify_set, former_test_set = data_set.get_data()\n",
    "true_train_set, true_verify_set, true_test_set = data_set.get_not_normaliza_data()\n",
    "\n",
    "# create model\n",
    "print(\"create model\")\n",
    "model = ACEFormer(data_dim=len(emd_col), embed_dim=64, forward_dim=256, unit_size=30, dis_layer=1, attn_layer=1, dropout=0.1, factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=tensor(0.2278), use time=3s, in 06-16 02:24:14, predict next epoch in 06-16 02:24:17\n"
     ]
    }
   ],
   "source": [
    "# train & test\n",
    "train_true_set, train_predict_set = [], []\n",
    "verify_true_set, verify_predict_set = [], []\n",
    "test_true_set, test_predict_set = [], []\n",
    "for i in range(backtest_num):\n",
    "    # training model with train set\n",
    "    model = train(model, former_train_set[i], batch_size, device, iteration)\n",
    "    # training\n",
    "    true, predict = test(model, former_train_set[i], predict_size=5, device=device)\n",
    "    train_true_set.append(true)\n",
    "    train_predict_set.append(predict)\n",
    "    # verify\n",
    "    true, predict = test(model, former_verify_set[i], predict_size=5, device=device)\n",
    "    verify_true_set.append(true)\n",
    "    verify_predict_set.append(predict)\n",
    "    # test\n",
    "    true, predict = test(model, former_test_set[i], predict_size=5, device=device)\n",
    "    test_true_set.append(true)\n",
    "    test_predict_set.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 1), (96, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.shape, predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
